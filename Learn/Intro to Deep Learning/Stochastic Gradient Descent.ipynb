{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When first created, all of the networks weights are set randomly, the network doesnÂ´t know anything yet. In this lesson we're going to see how to train a neural network, we're going to see how neural networks learn\n",
    "\n",
    "As with all machine learning tasks, we begin with a set of training data. Each example in the training data consists of some features together with an expected target. Training a network means adjusting its weights in such a way that it can transform the features into the target.\n",
    "\n",
    "In addition to the training data, we need two more things:\n",
    "    A 'loss function' that measures how good the network's predctions are\n",
    "    An 'optmizer' that can tell the network how to change its weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function:\n",
    "    We've seen how to design an architecture for a network, but we haven't seen how to tell a network what problem to solve. This is the job of thee loss function.\n",
    "\n",
    "    The loss function measures the disparity between the target's true value and the value the models predict\n",
    "\n",
    "    Different problems call for different loss function. We have been looking ate regression problems, where the task is to predict some numerical value.\n",
    "\n",
    "    A common loss function for regression problems is the mean absolute error or MAE. For each predction y_pred, MAE measures the disparity from the true target y_true by an absolute difference abs(y_true - y_predict).\n",
    "\n",
    "    Other loss functions you might see for regression problemas are the mean squared error(MSE) or the Hubber loss(both available in Keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Optimizer - Stochastic Gradient Descent:\n",
    "    We've described the problem we want the network to solve, but we need to say how to solve it. This is the job of the optmizer. The optimizer is an algorithm that adjust the weight to minimize the loss.\n",
    "\n",
    "    Virtually all of the optmization algorithms used in deep learning belong to a family called stochatisc gradient descent. They are iterative algorithms that train a network in steps. One step of training goes like this:\n",
    "        1.Sample some training data and run it through the network to make predictions.\n",
    "        2.Measure the loss between the predictions and the true values.\n",
    "        3.Finally, adjust the weights in a direction that makes the loss smaller.\n",
    "    Then just do this over and over until the loss is small as you like.\n",
    "    Each iteration's sample of training data is called minibatch(or just bacth) while a complete round of training data its called an epoch. The number of epochs you train is how many times the network will see each training example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Rate and Batch Size:\n",
    "    Notice that the line only makes a small shift in the direction of each batch (instead of moving all the way). The size of these shifts is determined by the learning rate. A smaller learning rate means the network needs to see more minibatches before its weights converge to their best values.\n",
    "\n",
    "    The learning rate and the size of the minibatches are the two parameters that have the largest effect on how the SGD training proceeds. Their interaction is often subtle and the right choice for these parameters isn't always obvious. (We'll explore these effects in the exercise.)\n",
    "\n",
    "    Fortunately, for most work it won't be necessary to do an extensive hyperparameter search to get satisfactory results. Adam is an SGD algorithm that has an adaptive learning rate that makes it suitable for most problems without any parameter tuning (it is \"self tuning\", in a sense). Adam is a great general-purpose optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the Loss and the Optimizer\n",
    "    model.compile(\n",
    "        optimizer ='adam',\n",
    "        loss='mae',\n",
    "    )\n",
    "Notice that we are able to specify the loss and optimizer with just a string. You can also access these directly through the Keras API -- if you wanted to tune parameters, for instance -- but for us, the defaults will work fine.\n",
    "\n",
    "What's In a Name?\n",
    "The gradient is a vector that tells us in what direction the weights need to go. More precisely, it tells us how to change the weights to make the loss change fastest. We call our process gradient descent because it uses the gradient to descend the loss curve towards a minimum. Stochastic means \"determined by chance.\" Our training is stochastic because the minibatches are random samples from the dataset. And that's why it's called SGD!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(512, activation='relu', input_shape=[11]),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(1),\n",
    "])\n",
    "\n",
    "Deciding the architecture of your model should be part of a process. Start simple and use the validation loss as your guide. You'll learn more about model development in the exercises.\n",
    "\n",
    "After defining the model, we compile in the optimizer and loss function.\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mae',\n",
    ")\n",
    "\n",
    "Now we're ready to start the training! We've told Keras to feed the optimizer 256 rows of the training data at a time (the batch_size) and to do that 10 times all the way through the dataset (the epochs).\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    batch_size=256,\n",
    "    epochs=10,\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "convert the training history to a dataframe\n",
    "history_df = pd.DataFrame(history.history)\n",
    "use Pandas native plot method\n",
    "history_df['loss'].plot();\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
