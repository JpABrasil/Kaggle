{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal and Noise:\n",
    "You might think about the information in the training data as being of two kinds: signal and noise. The signal is the part that generalizes, the part that can help our model make predictions from new data. The noise is that part that is only true of the training data; the noise is all of the random fluctuation that comes from data in the real-world or all of the incidental, non-informative patterns that can't actually help the model make predictions. The noise is the part might look useful but really isn't.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capacity: \n",
    "A model's capacity refers to the size and complexity of the patterns it is able to learn. For neural networks, this will largely be determined by how many neurons it has and how they are connected together. If it appears that your network is underfitting the data, you should try increasing its capacity\n",
    "\n",
    "You can increase the capacity of a network either by making it wider (more units to existing layers) or by making it deeper (adding more layers). Wider networks have an easier time learning more linear relationships, while deeper networks prefer more nonlinear ones. Which is better just depends on the dataset.\n",
    "\n",
    "    model = keras.Sequential([\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(1),\n",
    "    ])\n",
    "\n",
    "    wider = keras.Sequential([\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1),\n",
    "    ])\n",
    "\n",
    "    deeper = keras.Sequential([\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(1),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early Stopping:\n",
    "We mentioned that when a model is too eagerly learning noise, the validation loss may start to increase during training. To prevent this, we can simply stop the training whenever it seems the validation loss isn't decreasing anymore. Interrupting the training this way is called early stopping.\n",
    "\n",
    "Once we detect that the validation loss is starting to rise again, we can reset the weights back to where the minimum occured. This ensures that the model won't continue to learn noise and overfit the data.\n",
    "\n",
    "Training with early stopping also means we're in less danger of stopping the training too early, before the network has finished learning signal. So besides preventing overfitting from training too long, early stopping can also prevent underfitting from not training long enough. Just set your training epochs to some large number (more than you'll need), and early stopping will take care of the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Earlty Stopping:\n",
    "In Keras, we include early stopping in our training through a callback. A callback is just a function you want run every so often while the network trains. The early stopping callback will run after every epoch. (Keras has a variety of useful callbacks pre-defined, but you can define your own, too.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    min_delta=0.001, # minimium amount of change to count as an improvement\n",
    "    patience=20, # how many epochs to wait before stopping\n",
    "    restore_best_weights=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a model with Early Stopping:\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers, callbacks\n",
    "\n",
    "    early_stopping = callbacks.EarlyStopping(\n",
    "        min_delta=0.001, # minimium amount of change to count as an improvement\n",
    "        patience=20, # how many epochs to wait before stopping\n",
    "        restore_best_weights=True,\n",
    "    )\n",
    "\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(512, activation='relu', input_shape=[11]),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dense(1),\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='mae',\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_valid, y_valid),\n",
    "        batch_size=256,\n",
    "        epochs=500,\n",
    "        callbacks=[early_stopping], # put your callbacks in a list\n",
    "        verbose=0,  # turn off training log\n",
    "    )\n",
    "\n",
    "    history_df = pd.DataFrame(history.history)\n",
    "    history_df.loc[:, ['loss', 'val_loss']].plot();\n",
    "    print(\"Minimum validation loss: {}\".format(history_df['val_loss'].min()))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
